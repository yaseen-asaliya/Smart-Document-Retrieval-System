{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Document Retrieval System\n",
    "\n",
    "The objective of this project is to create and implement an information retrieval system utilizing Elasticsearch for document indexing and retrieval. The focus involves extracting temporal expressions and georeferences from documents to enable spatiotemporal and textual queries. Users can search for information based on time-related, geographical aspects, and traditional textual queries. This comprehensive approach enhances the system's capability to handle a wide range of queries, making it a powerful tool for information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from bs4 import BeautifulSoup\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch Connection\n",
    "Connect to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticsearch_host = 'localhost'\n",
    "elasticsearch_port = 9200\n",
    "\n",
    "es = Elasticsearch([f'http://{elasticsearch_host}:{elasticsearch_port}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Server Connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Connection to Elasticsearch failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting & Cleaning Data\n",
    "### Data Collecting\n",
    "Assign the zip file path as `zip_file` and the location for extracting the files as `extract_files_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = r'C:\\\\Users\\\\yasee\\Downloads\\\\archive (1).zip'\n",
    "extract_files_path = 'C:\\\\Users\\\\yasee\\\\Downloads\\\\extracted_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unzip_data_file` function takes the path for the folder that contains data, then exteact all the files in that path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted files to C:\\Users\\yasee\\Downloads\\extracted_data\n"
     ]
    }
   ],
   "source": [
    "def unzip_data_file(zip_path, extract_path):\n",
    "    try:\n",
    "        os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "        print(f\"Successfully extracted files to {extract_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")\n",
    "\n",
    "unzip_data_file(zip_path, extract_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `extract_reuters` function takes the path to the extracted files of type `sgm`, extracts all the Reuters elements, and then returns them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extract ruters.\n",
      "We have 21578 reuters.\n"
     ]
    }
   ],
   "source": [
    "def extract_reuters(extract_files_path):\n",
    "    reuters = []\n",
    "    try:\n",
    "        for file in os.listdir(extract_files_path):\n",
    "            if file.endswith(\".sgm\"):\n",
    "                filename = os.path.join(extract_files_path, file)\n",
    "                \n",
    "                with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    data_file = f.read()\n",
    "\n",
    "                soup = BeautifulSoup(data_file, 'html.parser')\n",
    "                reuters.extend(soup.find_all('reuters'))\n",
    "            \n",
    "        print(f\"Successfully extract ruters.\")\n",
    "        return reuters\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extracting ruters: {e}\") \n",
    "        \n",
    "reuters = extract_reuters(extract_files_path)\n",
    "print(f\"We have {len(reuters)} reuters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "The `split_authors` function takes authors as a string, then splits the string using `and` or `by` as separators. It subsequently removes extra whitespaces from the beginning and end of the string. Each author is then stored as an object containing `Firstname` and `Surname`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_authors(authors):\n",
    "    unclean_author_list = re.split(r'\\b(?:BY|AND)\\b', authors, flags=re.IGNORECASE)\n",
    "    clean_author_list = [author.strip() for author in unclean_author_list if author.strip()]\n",
    "    \n",
    "    authors_list = []\n",
    "    for author in clean_author_list:\n",
    "        author_parts = author.split(',')[0].split(' ')\n",
    "        authors_list.append({\"Firstname\": author_parts[0], \"Surname\": author_parts[1]})\n",
    "        \n",
    "    return authors_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `extreact_entitys` function takes all reuters, then extract all needed entitys, then stored them in articles list as an objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '26-FEB-1987 15:01:01.79', 'topics': 'cocoa', 'places': 'el-salvadorusauruguay', 'title': 'BAHIA COCOA REVIEW', 'author': 'N/A', 'dateline': '    SALVADOR, Feb 26 - ', 'body': 'Showers continued throughout the week in\\nthe Bahia cocoa zone, alleviating the drought since early\\nJanuary and improving prospects for the coming temporao,\\nalthough normal humidity levels have not been restored,\\nComissaria Smith said in its weekly review.\\n    The dry period means the temporao will be late this year.\\n    Arrivals for the week ended February 22 were 155,221 bags\\nof 60 kilos making a cumulative total for the season of 5.93\\nmln against 5.81 at the same stage last year. Again it seems\\nthat cocoa delivered earlier on consignment was included in the\\narrivals figures.\\n    Comissaria Smith said there is still some doubt as to how\\nmuch old crop cocoa is still available as harvesting has\\npractically come to an end. With total Bahia crop estimates\\naround 6.4 mln bags and sales standing at almost 6.2 mln there\\nare a few hundred thousand bags still in the hands of farmers,\\nmiddlemen, exporters and processors.\\n    There are doubts as to how much of this cocoa would be fit\\nfor export as shippers are now experiencing dificulties in\\nobtaining +Bahia superior+ certificates.\\n    In view of the lower quality over recent weeks farmers have\\nsold a good part of their cocoa held on consignment.\\n    Comissaria Smith said spot bean prices rose to 340 to 350\\ncruzados per arroba of 15 kilos.\\n    Bean shippers were reluctant to offer nearby shipment and\\nonly limited sales were booked for March shipment at 1,750 to\\n1,780 dlrs per tonne to ports to be named.\\n    New crop sales were also light and all to open ports with\\nJune/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\\nunder New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\\nper tonne FOB.\\n    Routine sales of butter were made. March/April sold at\\n4,340, 4,345 and 4,350 dlrs.\\n    April/May butter went at 2.27 times New York May, June/July\\nat 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\\n2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\\n2.27 times New York Dec, Comissaria Smith said.\\n    Destinations were the U.S., Covertible currency areas,\\nUruguay and open ports.\\n    Cake sales were registered at 785 to 995 dlrs for\\nMarch/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\\nNew York Dec for Oct/Dec.\\n    Buyers were the U.S., Argentina, Uruguay and convertible\\ncurrency areas.\\n    Liquor sales were limited with March/April selling at 2,325\\nand 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\\nYork July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\\nSept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\\nsaid.\\n    Total Bahia sales are currently estimated at 6.13 mln bags\\nagainst the 1986/87 crop and 1.06 mln bags against the 1987/88\\ncrop.\\n    Final figures for the period to February 28 are expected to\\nbe published by the Brazilian Cocoa Trade Commission after\\ncarnival which ends midday on February 27.\\n Reuter\\n\\x03'}\n"
     ]
    }
   ],
   "source": [
    "def extreact_entitys(reuters):\n",
    "    articles = []\n",
    "    for reuter in reuters:\n",
    "        \n",
    "        article = {\n",
    "            'date': reuter.find('date').text if reuter.find('date') else \"N/A\",\n",
    "            'topics': reuter.find('topics').text if reuter.find('topics') else \"N/A\",\n",
    "            'places': reuter.find('places').text if reuter.find('places') else \"N/A\",\n",
    "            'title': reuter.find('title').text if reuter.find('title') else \"N/A\",\n",
    "            'author': split_authors(reuter.find('author').text) if reuter.find('author') else \"N/A\",\n",
    "            'dateline': reuter.find('dateline').text if reuter.find('dateline') else \"N/A\",\n",
    "            'body': reuter.find('body').text if reuter.find('body') else \"N/A\"\n",
    "        }\n",
    "        \n",
    "        articles.append(article)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "articles = extreact_entitys(reuters)\n",
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dateutil import parser\n",
    "\n",
    "original_date_str = \"26-FEB-1987 15:02:20.00\"\n",
    "parsed_date = parser.parse(original_date_str)\n",
    "formatted_date_str = parsed_date.strftime(\"%Y-%m-%d %H:%M:%S \")\n",
    "print(formatted_date_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
