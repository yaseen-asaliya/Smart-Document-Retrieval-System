{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Document Retrieval System\n",
    "\n",
    "The objective of this project is to create and implement an information retrieval system utilizing Elasticsearch for document indexing and retrieval. The focus involves extracting temporal expressions and georeferences from documents to enable spatiotemporal and textual queries. Users can search for information based on time-related, geographical aspects, and traditional textual queries. This comprehensive approach enhances the system's capability to handle a wide range of queries, making it a powerful tool for information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from nltk.stem import PorterStemmer\n",
    "from dateutil import parser\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderUnavailable\n",
    "import time\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch Connection\n",
    "Connect to the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticsearch_host = 'localhost'\n",
    "elasticsearch_port = 9200\n",
    "\n",
    "es = Elasticsearch([f'http://{elasticsearch_host}:{elasticsearch_port}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Server Connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch\n"
     ]
    }
   ],
   "source": [
    "if es.ping():\n",
    "    print(\"Connected to Elasticsearch\")\n",
    "else:\n",
    "    print(\"Connection to Elasticsearch failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting & Cleaning Data\n",
    "### Data Collecting\n",
    "Assign the zip file path as `zip_file` and the location for extracting the files as `extract_files_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = 'C:\\\\Users\\\\yasee\\Downloads\\\\archive (1).zip'\n",
    "extract_files_path = 'C:\\\\Users\\\\yasee\\\\Downloads\\\\extracted_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unzip_data_file` function takes the path for the folder that contains data, then exteact all the files in that path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted files to C:\\Users\\yasee\\Downloads\\extracted_data\n"
     ]
    }
   ],
   "source": [
    "def unzip_data_file(zip_path, extract_path):\n",
    "    try:\n",
    "        os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "        print(f\"Successfully extracted files to {extract_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extraction: {e}\")\n",
    "\n",
    "unzip_data_file(zip_path, extract_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `extract_reuters` function takes the path to the extracted files of type `sgm`, extracts all the Reuters elements, and then returns them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extract ruters.\n",
      "We have 21578 reuters.\n"
     ]
    }
   ],
   "source": [
    "def extract_reuters(extract_files_path):\n",
    "    reuters = []\n",
    "    try:\n",
    "        for file in os.listdir(extract_files_path):\n",
    "            if file.endswith(\".sgm\"):\n",
    "                filename = os.path.join(extract_files_path, file)\n",
    "                \n",
    "                with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    data_file = f.read()\n",
    "\n",
    "                soup = BeautifulSoup(data_file, 'html.parser')\n",
    "                reuters.extend(soup.find_all('reuters'))\n",
    "            \n",
    "        print(f\"Successfully extract ruters.\")\n",
    "        return reuters\n",
    "    except Exception as e:\n",
    "        print(f\"Error during extracting ruters: {e}\") \n",
    "        \n",
    "reuters = extract_reuters(extract_files_path)\n",
    "print(f\"We have {len(reuters)} reuters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Load `spaCy` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_format(date):\n",
    "    try:\n",
    "        parsed_date = parser.parse(date[:22].strip())\n",
    "        return parsed_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date: {e}\") \n",
    "        return \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_body_content(body):\n",
    "    tokens = nlp(body)\n",
    "\n",
    "    tokens = [token.text for token in tokens if not token.is_stop and len(token.text) >= 3]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `extract_authors` function takes authors as a string, then splits the string using `and` or `by` as separators. It subsequently removes extra whitespaces from the beginning and end of the string. Each author is then stored as an object containing `firstname` and `surname`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'firstname': 'Janie', 'surname': 'Gabbett'}]\n",
      "[{'firstname': 'Janie', 'surname': 'Gabbett'}, {'firstname': 'Mike', 'surname': 'Ross'}]\n"
     ]
    }
   ],
   "source": [
    "def extract_authors(authors):\n",
    "    unclean_author_list = re.split(r'\\b(?:BY|AND)\\b', authors, flags=re.IGNORECASE)\n",
    "    clean_author_list = [author.strip() for author in unclean_author_list if author.strip()]\n",
    "\n",
    "    authors_list = []\n",
    "    \n",
    "    for author in clean_author_list:\n",
    "        author_parts = author.split(',')[0].split(' ')\n",
    "        authors_list.append({\"firstname\": author_parts[0], \"surname\": author_parts[1]})\n",
    "        \n",
    "    return authors_list\n",
    "\n",
    "# Usage\n",
    "print(extract_authors(\"    by Janie Gabbett, Reuters\"))\n",
    "print(extract_authors(\"    by Janie Gabbett and Mike Ross, Reuters\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `extract_d_elements` function takes a tag that contais elemts in tag `D`, then extract all these elements as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_d_elements(d_elements):\n",
    "    if d_elements:\n",
    "        d_elements = d_elements.find_all('d')\n",
    "        return [element.text for element in d_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text().strip().replace(\"  \",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_coordinates(places):\n",
    "    list_of_places_corrd = []\n",
    "    try:\n",
    "        api_url = \"https://nominatim.openstreetmap.org/search?format=json&q=\" + \"%20\".join(places)\n",
    "\n",
    "        response = requests.get(api_url)\n",
    "        data = json.loads(response.text)\n",
    "\n",
    "        for place in data:\n",
    "            if(place['addresstype'] == \"city\" or place['addresstype'] == \"state\"):\n",
    "                list_of_places_corrd.append({\n",
    "                    \"lat\": place['lat'], \n",
    "                    \"lon\": place['lon']\n",
    "                })\n",
    "        return list_of_places_corrd\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to retruive corrdinates: {e}\")\n",
    "        get_coordinates(places)\n",
    "    return {\"lat\": 0, \"lon\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_expressions(text, category):\n",
    "    doc = nlp(text)\n",
    "    temporal_expressions = [ent.text for ent in doc.ents if ent.label_ == category]\n",
    "\n",
    "    return temporal_expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lat': '34.0536909', 'lon': '-118.242766'}, {'lat': '8.524167250000001', 'lon': '-82.19418575966449'}, {'lat': '-37.4707455', 'lon': '-72.351686'}]\n"
     ]
    }
   ],
   "source": [
    "def extract_dateline_countrys(dateline):\n",
    "    \n",
    "    countrys = extract_temporal_expressions(nlp(dateline), \"GPE\")  # Geo-Political Entity\n",
    "\n",
    "    return get_coordinates(countrys)\n",
    "# Usage \n",
    "print(extract_dateline_countrys(\"    LOS ANGELES, Feb 26 - \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_for_places(places):\n",
    "\n",
    "    places = extract_d_elements(places)\n",
    "    \n",
    "    return get_coordinates(places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_geopoints(body):\n",
    "    places = extract_temporal_expressions(nlp(body), \"GPE\")\n",
    "    \n",
    "    places = list(set(places))\n",
    "    return get_coordinates(places)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `approximate_geopoints` function reutrn the most occurences geopoint if there wasn't a country or city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'lat': 12.123, 'lon': 1.4213}]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def approximate_geopoints(georeferences, countrys_coordinates, citys_coordinates):\n",
    "\n",
    "    if len(countrys_coordinates) == 0 and len(citys_coordinates) == 0 and not len(georeferences) == 0:\n",
    "        # Counter to count occurrences of each georeference\n",
    "        georeference_counts = Counter(tuple(georef.items()) for georef in georeferences)\n",
    "        most_common_georeference = dict(georeference_counts.most_common(1)[0][0])\n",
    "        \n",
    "        return [{\"lat\": most_common_georeference['lat'], \"lon\": most_common_georeference['lon']}]\n",
    "    else:\n",
    "        return [countrys_coordinates] + [citys_coordinates]\n",
    "\n",
    "# Usage\n",
    "georeferences_list = [{\"lat\": 0, \"lon\": 0}, {\"lat\": 12.123, \"lon\": 1.4213}, {\"lat\": 12.123, \"lon\": 1.4213}, {\"lat\": 2, \"lon\": 2}]\n",
    "print(approximate_geopoints(georeferences_list, [], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_expression_as_dict(reuter):\n",
    "    if reuter.find('body'):\n",
    "        temproals = extract_temporal_expressions(reuter.find('body').text, \"DATE\")\n",
    "        return [{ \"expression\" : temp } for temp in temproals]\n",
    "    else:\n",
    "        { \"expression\": \"N/A\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"date\": {\"type\": \"date\"},\n",
    "            \"topics\": {\"type\": \"keyword\"},\n",
    "            \"title\": {\"type\": \"text\", \"analyzer\": \"autocomplete\", \"search_analyzer\": \"autocomplete_search\"},\n",
    "            \"author\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"firstname\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\"}}},\n",
    "                    \"surname\": {\"type\": \"text\", \"fields\": {\"keyword\": {\"type\": \"keyword\"}}}\n",
    "                }\n",
    "            },\n",
    "            \"analized-body\": {\"type\": \"text\"},\n",
    "            \"body\": {\"type\": \"text\"},\n",
    "            \"temporal-expression\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\"type\": \"text\"}\n",
    "                }\n",
    "            },\n",
    "            \"geopoints\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"lon\": {\"type\": \"double\"},\n",
    "                    \"lat\": {\"type\": \"double\"}\n",
    "                }\n",
    "            },\n",
    "            \"georeferences\": {\n",
    "                \"type\": \"nested\",\n",
    "                \"properties\": {\n",
    "                    \"lon\": {\"type\": \"double\"},\n",
    "                    \"lat\": {\"type\": \"double\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"autocomplete\": {\n",
    "                    \"tokenizer\": \"autocomplete\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                },\n",
    "                \"autocomplete_search\": {\n",
    "                    \"tokenizer\": \"lowercase\"\n",
    "                }\n",
    "            },\n",
    "            \"tokenizer\": {\n",
    "                \"autocomplete\": {\n",
    "                    \"type\": \"edge_ngram\",\n",
    "                    \"min_gram\": 3,\n",
    "                    \"max_gram\": 10,\n",
    "                    \"token_chars\": [\"letter\", \"digit\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"smart_document_system\"\n",
    "\n",
    "if not es.indices.exists(index=index_name):\n",
    "    es.indices.create(index=index_name, body=index_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of indexed articles is 21579.\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "\n",
    "for reuter in reuters: \n",
    "    citys_coordinates = extract_dateline_countrys(reuter.find('dateline').text) if reuter.find('dateline') else []\n",
    "    countrys_coordinates = get_coordinates_for_places(reuter.find('places')) if reuter.find('places') else []\n",
    "    georeferences = extract_geopoints(reuter.find('body').text) if reuter.find('body') else [{ \"lat\": 0, \"lon\": 0 }]    \n",
    "    \n",
    "    actions.append({\n",
    "        \"_op_type\": \"index\",\n",
    "        \"_index\": index_name,\n",
    "        \"_source\": {\n",
    "            'date': convert_date_format(reuter.find('date').text) if reuter.find('date') else \"N/A\",\n",
    "            'topics': extract_d_elements(reuter.find('topics')) if reuter.find('topics') else \"N/A\",\n",
    "            'title': clean_title(reuter.find('title').text) if reuter.find('title') else \"N/A\",\n",
    "            'author': extract_authors(reuter.find('author').text) if reuter.find('author') else { \"firstname\": \"N/A\", \"surname\": \"N/A\" }, \n",
    "            'analized-body': extract_body_content(reuter.find('body').text) if reuter.find('body') else \"N/A\",\n",
    "            'body': reuter.find('body').text if reuter.find('body') else \"N/A\",  # For frontend purpose\n",
    "            'temporal-expression': extract_temporal_expression_as_dict(reuter) if reuter.find('body') else { \"expression\": \"N/A\" },\n",
    "            'geopoints': approximate_geopoints(georeferences, countrys_coordinates, citys_coordinates), \n",
    "            'georeferences': georeferences\n",
    "            }\n",
    "    })\n",
    "    \n",
    "    if len(actions) == 500:   \n",
    "        success, failed = bulk(es, actions)\n",
    "        count_result = es.count(index=index_name)\n",
    "        indexed_docs = count_result['count']\n",
    "        \n",
    "        print(f\"Successfully indexed new {success} articles.\")\n",
    "        print(f\"Number of indexed articles is {indexed_docs} until now.\")\n",
    "        actions = []\n",
    "        \n",
    "\n",
    "if actions:\n",
    "    success, failed = bulk(es, actions)\n",
    "    count_result = es.count(index=index_name)\n",
    "    indexed_docs = count_result['count']\n",
    "\n",
    "    print(f\"Successfully indexed new {success} articles.\")\n",
    "    print(f\"Total number of indexed articles is {indexed_docs}.\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
